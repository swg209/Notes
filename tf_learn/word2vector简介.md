## Intro

全称SVD(Singular Value Decomposition)，奇异值分解

## 准备知识

###  对矩阵的认识

$$Av = \lambda v$$

特征向量$v$都有对应的特征值$\lambda$，实对称矩阵不同特征值对应的特征向量是正交的。

特征值分解可得到

$$A=U\Lambda U^T$$

首先，要明确的是，**一个矩阵其实是一个线性变换**，因为一个向量跟矩阵相乘，相当于对矩阵做了线性变换，也就是伸缩、翻转、平移等等操作。

但是特征值表示什么？查资料的时候有篇博客说，如果给特征值从大到小排序，**最大的特征值对应的特征向量描述了这个矩阵最主要的变化方向**（从主要变化到次要变化排列）
