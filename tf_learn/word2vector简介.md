## Intro

全称SVD(Singular Value Decomposition)，奇异值分解

### 准备知识 对矩阵的认识

$$Av = \lambda v$$

特征向量$v$都有对应的特征值$\lambda$，实对称矩阵不同特征值对应的特征向量是正交的。

特征值分解可得到

$$A=U\Lambda U^T$$

首先，要明确的是，**一个矩阵其实是一个线性变换**，因为一个向量跟矩阵相乘，相当于对矩阵做了线性变换，也就是伸缩、翻转、平移等等操作。

但是特征值表示什么？查资料的时候有篇博客说，如果给特征值从大到小排序，**最大的特征值对应的特征向量描述了这个矩阵最主要的变化方向**（从主要变化到次要变化排列）。在高维空间下的一个线性变换，如果我们通过特征值分解得到前N个特征向量，用这前N个特征向量就可以近似这个线性变换。

**总结：**

特征值表示这个特征有多重要，特征向量表示这个特征是什么。**由N个特征向量张成的空间可以描述由这N个特征组成的实体**（五官构成一张脸）。每一个特征向量代表一个线性的子空间，比如说，代表了眼睛的大小。但是，特征值分解有很多局限，比如**该变换矩阵必须是方阵**。现实世界中刚好是方阵的矩阵是不多的，所以，引入奇异值分解。

## 奇异值

$$A=U \Sigma V^T$$

$$A(m*n) = U(m*m) \times \Sigma(m*n) \times V^T(n*n)$$

U里面的向量称为**左奇异向量**；$\Sigma$对角矩阵，**对角线上的元素为奇异值**；V里面的向量称为**右奇异向量**

**计算奇异值**

这部分的推导有一步没看太懂。所以这里就不写了

**奇异值的意义**

当将一个变换矩阵A分解成$A=U\Sigma V^T$，那么$V$就是表示原空间的基，$U$表示经过转换后的空间的基，$\Sigma$中对角线上的每个值都表示经过转换后得到的空间的某个基向量方向上的值。比如说$x$是$x$轴的单位向量，那么$2x$就是在$x$轴这个方向上伸展长度2，类比过来，$x$就是$U$里面的向量，2就是$\Sigma$里面的某个奇异值。

再引用知乎上的回答 [lao king](https://www.zhihu.com/question/22237507/answer/28007137)

>我感觉，无论是特征值分解还是奇异值分解，都是为了让人们对矩阵（或者线性变换）的作用有一个直观的认识。这是因为我们拿过来一个矩阵，很多情况下只能看到一堆排列有序的数字，而看不到这些数字背后的真实含义，特征值分解和奇异值分解告诉了我们这些数字背后的真实含义，换句话说，它告诉了我们关于矩阵作用的本质信息。奇异值分解的含义是，把一个矩阵A看成线性变换（当然也可以看成是数据矩阵或者样本矩阵），那么这个线性变换的作用效果是这样的，我们可以在原空间找到一组标准正交基V，同时可以在像空间找到一组标准正交基U，我们知道，看一个矩阵的作用效果只要看它在一组基上的作用效果即可，在内积空间上，我们更希望看到它在一组标准正交基上的作用效果。而矩阵A在标准正交基V上的作用效果恰好可以表示为在U的对应方向上只进行纯粹的伸缩！这就大大简化了我们对矩阵作用的认识，因为我们知道，我们面前不管是多么复杂的矩阵，它在某组标准正交基上的作用就是在另外一组标准正交基上进行伸缩而已。特征分解也是这样的，也可以简化我们对矩阵的认识。对于可对角化的矩阵，该线性变换的作用就是将某些方向（特征向量方向）在该方向上做伸缩。有了上述认识，当我们要看该矩阵对任一向量x的作用效果的时候，在特征分解的视角下，我们可以把x往特征向量方向上分解，然后每个方向上做伸缩，最后再把结果加起来即可；在奇异值分解的视角下，我们可以把x往V方向上分解，然后将各个分量分别对应到U方向上做伸缩，最后把各个分量上的结果加起来即可。当我们注意到，不是所有的矩阵都能对角化（对称矩阵总是可以），而所有矩阵总是可以做奇异值分解的。那么多类型的矩阵，我们居然总是可以从一个统一且简单的视角去看它，我们就会感叹奇异值分解是多么奇妙了！
