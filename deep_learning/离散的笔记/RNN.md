# Why RNN

某些任务是需要NN具有记忆的，比如

* 我 来了 台北
* 我 离开了 台北

要确定我要离开还是来了台北，需要知道前一个单词是什么。这就需要网络具有记忆，于是就提出了RNN。

# Why RNN's error surface so rough

这是长距离传输引起的。假设有一个RNN，它的 memory 等于上一个时刻的 memory * w。除了第一个时刻 input = 1 之外，其他所有时刻 input = 0。那么在经过1001个时刻的传输后：

$$ w = 1，1*w^{1000} = 1 $$
$$ w = 1.01，1*w^{1000} = 20000 $$
$$ w = 0.99，1*w^{1000} = 0 $$
$$ w = 0.01，1*w^{1000} = 0 $$

可以看到，w发生微小的变化会引起蝴蝶效应。这是长距离传输里面存在的问题，也是 error surface 抖动如此大的原因所在。

当然，这是李宏毅视频里面说的，我的问题是，这明显是一个单调函数，如何会引起剧烈抖动？

可能跟 training 过程中参数的微小变化有关。但是参数存在以下变动情况：

* 参数经常有微小变化。这个应该不会形成蝴蝶效应吧？
* 参数单调增减一段时间又单调减增。不明。
* 参数总体是单调的。这种情况应该是逐渐变化的，不会出现剧烈抖动吧？
* 参数经常剧烈变化，即 Gradient 值突然很大。根据前面视频的内容，RNN应该就是这种情况。只是如果是这种情况，他举上面那个长距离传输例子既不是都不相关了吗？

后来我通过编程发现，```result *= w```这种运算一开始 ```result(t)-result(t-1)```只有0.几，但是到后面就有100+。

**所以**，这确实是原因所在，训练越到后面，Gradient 的值变化越来越大，error surface 的动荡越频繁越明显。所以作者想了下面的解决办法。

**how to solve**

由于RNN的 error surface 太陡峭，所以训练的时候有个技巧，就是当 Gradient 超过某个 threshold 的时候就不要让它超过那个 threshold 。比如说超过了15，就让它 = 15。这样，参数的变化就会相对不会起飞太严重。

# RNN的training trick

### Identity matrix + ReLU

对于一般的RNN来说，可以用单位矩阵+ReLU来初始化RNN，会有很好的效果

---
# LSTM

### 具体实现用到的技巧

假设它的三个 GATE 的输入都是线性的，例如对 input-gate 来说，y = a1x1+a2x2+a3x3+a4b。它可以通过令 a4 = -10 来使 input-gate 有一定的阈值。

### LSTM trick


---
# GRU

如果发现LSTM过拟合很严重，可以试试这个。


---
# Gradient vanished

提到上面的问题顺便提下 Gradient vanished

是 sigmoid 导致的。因为 sigmoid 的值在 [0, 1] 之间，所以在很深的网络里面会使每次的值越来越小，从而出现 vanished。

##### 其实，Rough 和 Vanished 本质上都是长距离传输导致的问题。
