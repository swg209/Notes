### 从头开始的简短介绍

决策树通过某种方法构建树，用树做分类。所以核心是：

1. 如何决定非叶子节点(parent)
2. 何时产生新的分支

西瓜书介绍的各种解决以上问题的方案，都是以信息论为基础出发的。所以先来搞清楚信息论的基本概念。

- [自信息的定义](https://zh.wikipedia.org/wiki/%E8%87%AA%E4%BF%A1%E6%81%AF)；[为什么会有log](https://www.zhihu.com/question/30828247/answer/64816509)：准确地描述了自信息如此定义的原因，以及为何会有负号
- [熵](https://zh.wikipedia.org/wiki/%E7%86%B5_(%E4%BF%A1%E6%81%AF%E8%AE%BA))：通俗来说，即随机变量的各种取值的自信息的期望

现在可以描述信息的多少了，但是还需要描述变化：

- [信息增益](https://en.wikipedia.org/wiki/Information_gain_in_decision_trees)：因决策树而产生的概念，决策树根据它来决定用哪个属性作为分支点。
- [gain ratio](https://en.wikipedia.org/wiki/Information_gain_ratio)：

还有方法从另一个角度制定选取划分属性的标准：

- [Gini index]()：用另一个角度，即纯度，来描述划分

这三个各有各的缺点。

- 信息增益由于有$\frac{|D^v|}{|D|}$，所以会**对$v$比较大的属性有所偏好**
- 增益率由于有$IV(a)=-\sum_{v=1}^V\frac{|D^v|}{|D|}log_2\frac{|D^v|}{|D|}$，$V$越大，$IV(a)$的值通常会越大，所以会**对$v$比较小的属性有所偏好**
-


---

### Intro

- 划分选择
    - 信息熵(information entropy)
    - 信息增益(information gain) ID3
    - 增益率(gain ratio) C4.5
    - 基尼系数(Gini_index) CART

### 划分选择

目的：分支节点所包含的样本尽可能属于同一类别，即“**纯度**”**(purity)** 越来越高。

那么如何衡量纯度？用**信息熵(information entropy)** 。信息熵起源于物理学，是用于描述不确定性。在信息论中，它也同样是对不确定性的测量，换句话说，即是描述了某个随机变量$X$所包含的信息量。当$X$的不确定性越高，其能包含的信息量就越大，熵也越大。定义为随机变量$X$的信息量$I(X)$的均值，即

$$H(X) = E(I(X)) = E[-ln(P(X))]$$

当取自有限样本的时候，就可以写成离散形式

$$H(X) = \sum_iP(x_i)I(x_i) = -\sum_i P(x_i)log_bP(x_i)$$

**回到决策树，我们可以根据信息熵越小，纯度越大来划分样本。为了统一各个属性的参考标准以及处理方法，我们使用信息增益(information gain)**

$$Gain(D, a) = H(X) - \sum_{v=1}^{V} \frac{|D^v|}{D} H(D^v)$$

其中

- $D$表示整个集合
- $v$表示属性$a$有$v$个取值
- $D^v$表示所有属性$a$取值为$a^v$的样本
- 整个式子描述了以属性$a$对样本$D$进行划分所带来的信息增益

**ID3** (Iterative Dichotomiser，迭代二分器)使用的就是信息增益：

1. 计算各个属性的信息增益
2. 选最大的一个作为划分属性；若有多个最大的，选一个即可
3. 重复1和2直到划分完所有属性

**不过，从$Gain(\cdot)$可以看出，信息增益准则有可能会倾向于取值比较多的属性**，这个是有可能，不一定，可以理解为，属性取值越多，则每个取值下的集合越小，纯度会相对提高，不确定性减少，信息增益会相对增大。不过再次强调一下，这个不是一定的，只是有这个隐患。

为了解决这个隐患，改用**增益率**(gain ratio)

$$Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}$$

$$IV(a) = -\sum_{v=1}^{V} \frac{|D^v|}{|D|} log_2\frac{|D^v|}{|D|}$$

$a$指某个属性。$IV$实际上表示了属性$a$的各个取值占比的信息熵。**属性$a$的取值越多，$IV$越大**，这样就解决了上面提到的隐患。

**C4.5** 就是使用增益率gain ratio的决策树算法。

**不过，增益率准则可能偏向于取值数目比较少的属性值**。所以C4.5是先从候选划分属性中选择信息增益高于平均水平的属性，然后再从增益率中选择最高的。

我们还有别的**度量纯度的方法**，基尼值：

$$Gini(D) = \sum_{k=1}^{|y|} \sum_{k^{'} \not=k} p_k p_{k^{'}} = 1 - \sum_{k=1}^{|y|} p_{k}^2$$

基尼指数描述了从$D$中随机抽取两个样本，两个样本的类别不一样的概率。**所以，$Gini(\cdot)$越小，$D$的纯度越高**。

为了将$Gini$引入属性中，则有**基尼指数(Gini index)** ：

$$Gini\_index(D,a) = \sum_{v} \frac{|D^v|}{|D|}Gini(D^v)$$

对**CART**来说，便是使用Gini_index选择划分属性的。我们选择Gini_index最小的作为划分属性，即

$$a_* = argmin_{a \in A} Gini\_index(D,a)$$

>对了，这里想到一个问题。information gain为何不直接采用最后面的部分做argmin，而还要用总信息量减一次？

### 剪枝处理 pruning

- 用于对付过拟合

有**预剪枝**和**后剪枝**两种方法。

- 预剪枝：在展开分支之前，先计算展开前和展开后的test_acc，如果没提升，就不展开
- 后剪枝：在整个决策树搭建完成后，自下而上剪枝。若当前遍历到的根节点减掉后效果没提升，就不剪；有提升，则减掉

### 连续与缺失值

在实际处理中我们会碰到一些问题

**连续值处理**

不可能每个值给它划一个分支的。于是想了一个办法：

- 将所有两个连续值之间的中位点作为一个集合$T_a={\{ \frac{a^i+a^{i+1}}{2}|1 \le i \le n-1 \}}$，在集合中找一个点，使得其信息增益最大，公式如下：

$$Gain(D,a) = max_{t \in T_a} Gain(D,a,t)$$

$$Gain(D,a) = max_{t \in T_a} H(D)-\sum_{\lambda \in \{-,+\}} \frac{|D_t^{\lambda}|}{|D|}H(D^{\lambda}_t)$$

**缺失值处理**

略复杂，不作记录。大概思路讲的是现在某些属性的值有缺失，如何根据现有的情况计算信息增益。它的做法还是按原来的公式来，只不过只计算该属性有值的数据，并且引入了$\rho$调整因为没计算缺失值数据的影响，个人觉得这个$\rho$可能并不足够公平。

### 多变量决策树

即用多变量拟合决策树的划分边界，讲得不多
