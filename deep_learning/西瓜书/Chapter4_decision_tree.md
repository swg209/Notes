### Intro

- 划分选择
    - 信息熵(information entropy)
    - 信息增益(information gain)
    - 增益率(gain ratio)

### 划分选择

目的：分支节点所包含的样本尽可能属于同一类别，即“**纯度**”**(purity)** 越来越高。

那么如何衡量纯度？用**信息熵(information entropy)** 。信息熵起源于物理学，是用于描述不确定性。在信息论中，它也同样是对不确定性的测量，换句话说，即是描述了某个随机变量$X$所包含的信息量。当$X$的不确定性越高，其能包含的信息量就越大，熵也越大。定义为随机变量$X$的信息量$I(X)$的均值，即

$$H(X) = E(I(X)) = E[-ln(P(X))]$$

当取自有限样本的时候，就可以写成离散形式

$$H(X) = \sum_iP(x_i)I(x_i) = -\sum_i P(x_i)log_bP(x_i)$$

**回到决策树，我们可以根据信息熵越小，纯度越大来划分样本。为了统一各个属性的参考标准以及处理方法，我们使用信息增益(information gain)**

$$Gain(D, a) = H(X) - \sum_{v=1}^{V} \frac{|D^v|}{D} H(D^v)$$

其中

- $D$表示整个集合
- $v$表示属性$a$有$v$个取值
- $D^v$表示所有属性$a$取值为$a^v$的样本
- 整个式子描述了以属性$a$对样本$D$进行划分所带来的信息增益

**ID3** (Iterative Dichotomiser，迭代二分器)使用的就是信息增益：

1. 计算各个属性的信息增益
2. 选最大的一个作为划分属性；若有多个最大的，选一个即可
3. 重复1和2直到划分完所有属性

**不过，从$Gain(\cdot)$可以看出，信息增益准则有可能会倾向于取值比较多的属性**，这个是有可能，不一定，可以理解为，属性取值越多，则每个取值下的集合越小，纯度会相对提高，不确定性减少，信息增益会相对增大。不过再次强调一下，这个不是一定的，只是有这个隐患。

为了解决这个隐患，改用**增益率**(gain ratio)

$$Gain\_ratio(D,a) = \frac{Gain(D,a)}{IV(a)}$$

$$IV(a) = \sum_{v=1}^{V} \frac{|D^v|}{|D|} log_2\frac{|D^v|}{|D|}$$

$a$指某个属性。$IV$实际上表示了属性$a$的各个取值占比的信息熵。**属性$a$的取值越多，$IV$越大**，这样就解决了上面提到的隐患。

**C4.5** 就是使用增益率gain ratio的决策树算法。

**不过，增益率准则可能偏向于取值数目比较少的属性值**。所以C4.5是先从候选划分属性中选择信息增益高于平均水平的属性，然后再从增益率中选择最高的。

我们还有别的**度量纯度的方法**，基尼值：

$$Gini(D) = \sum_{k=1}^{|y|} \sum_{k^{'} \not=k} p_k p_{k^{'}} = 1 - \sum_{k=1}^{|y|} p_{k}^2$$

基尼指数描述了从$D$中随机抽取两个样本，两个样本的类别不一样的概率。**所以，$Gini(\cdot)$越小，$D$的纯度越高**。

为了将$Gini$引入属性中，则有**基尼指数(Gini index)** ：

$$Gini\_index(D,a) = \sum_{v} \frac{|D^v|}{|D|}Gini(D^v)$$

对**CART**来说，便是使用Gini_index选择划分属性的。我们选择Gini_index最小的作为划分属性，即

$$a_* = argmin_{a \in A} Gini\_index(D,a)$$

>对了，这里想到一个问题。information gain为何不直接采用最后面的部分做argmin，而还要用总信息量减一次？

### 剪枝处理 pruning

- 用于对付过拟合
