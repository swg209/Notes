## 2.2 评估方法

评估前，首先是划分数据集(train, val, test)

### 划分数据集

划分数据及的时候涉及到如何**sampling(采样)** ，这个问题有许多策略

- stratified sampling(分层采样)：保留类别比例的抽样方式，可避免train与test的数据分布差异导致的误差

划分的方式不同会导致训练结果不同。可以尝试若干次随机划分、重复实验然后取平均值作为结果。

另外，train与test**占总数据集的比例**也对结果有影响。比如：

- training data多，testing data少，testing data的分布可能就会与总分布不一致，导致test的结果不好
- training data少，testing data多，training data的分布可能就会与总分布不一致，model就会学得不好

### 验证法

#### cross validation(交叉验证)

K-flod cross validation(k折交叉验证)：将数据集D划分为k个等大小的互斥子集，每次取k-1个子集训练，剩下的1个做测试集；共训练k次，每个子集都当一回测试集。最后将k个结果去均值作为输出。**但是还没完！**，因为同一个数据集有多种划分子集的方式，所以需要用不同的划分方式做多次k-flod cv，比如常见有**10次10-flod cv**，然后将10次10-flod结果取均值，这才是最终输出。

- 优点：个人觉得挺好的
- 缺点：可能就在于比较繁琐，训练多次。不过次数自己定，挺灵活的。缺点不明显。

Leve-One-Out(留一法)：与k-flod一样，只不过这个方法的将一条数据作为一个子集

- 优点：比较准确，因为几乎把所有数据都作为了训练集
- 缺点：数据量大的时候开销非常大


#### Bootstrap(自助法)

*简而言之，是通过有放回抽样，近似得到样本的一些信息*

比如说，一个鱼塘鱼的条数未知，上帝视角有1000条。此时我做以下操作：

1. 抓100条上来，打标签，放回鱼塘
2. 等鱼混合充分了，抓100条上来
3. 放回去
4. 重复2、3步n次

第一次我们抓到的100条鱼中可能有7条有标签，占7/100；第二次可能有8条，占8/100。多次抽样取均值后，会发现打标签的鱼大概占样本的10%，然后我们可以推测鱼塘有1000条鱼。

*西瓜书* 里面讲得比较深：

设数据集D有m个样本，每次从D抽一个样本拷贝后放入_D，样本放回D。这个过程执行m次，那么某个样本在m次采样中不被采到的概率是$(1-\frac{1}{m})^m$，取极限：

$$\lim\limits_{m \to \infty} (1-\frac{1}{m})^m \mapsto \frac{1}{e} \approx 0.368$$

这里的意思是，初始数据集有36.8%的样本未出现在_D中，所以我们可以

将_D作为train
D除去_D作为test

这样，train和test都有m个样本，而且test里面有36.8%的样本从未出现在train。这样得到的测试结果，亦称为（out-of-bag estimate）

**问题是，0.368是m趋于无穷大的时候得到的，也就是初始数据集跟抽样次数都趋于无穷大，我明白他这里主要是想近似得到e故意设计的，所以我不确定这个结果在普通情况下是否适用。** 不过，作者也只是想表达这个方法还有这一层意思咯。

**总结**

- 优点：适用于数据集较小，难以有效划分train/test
- 缺点：抽样过程中，可能有些数据多次出现，有些则少些，那么所产生的数据集有可能就会和初始数据集的分布不一样，这会引入估计误差。所以数据量充足的时候，还是用cv。

#### Precision & Recall & F1

之前我一直很奇怪，
