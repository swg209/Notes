## 揭开迷雾，来一顿美味的「Capsule」盛宴

- 来源：[here](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487194&idx=1&sn=1be48dd9a45f7f3a35024fdce55f2369&chksm=96e9d15aa19e584c0b4815d6b5791a7ab859049116f8500a08c454c55457f9489cf2adf4ab93&scene=21%23wechat_redirect&pass_ticket=lmL8VwvCE9islElDAChRseZPB%2FtG9VYF%2FCRr82%2BET8Qx7hnfXx%2FjJMHlLsFfeCis)

文章将capsule的DR算法流程用迭代公式写出来，用一种不同的角度去看待capsule。

其主要观点如下：

- capsule的及核心思想就是 输出 是 输入 的某种聚类结果，也就是说，DR相当于一个聚类过程
- capsule的模长能够代表这个特征的概率，模长越大，这个特征越显著
- 对原来的压缩方案squashing做了修改，$squashing(x) = \frac{||x||^2}{0.5+||x||^2}\frac{x}{||x||}$，使得模长接近于0时起到放大作用，而不像原来的函数那样全局压缩
(这个我存疑，因为这一步得到的结果决定了b，进而决定了c。相关性小的特征我认为还是尽快趋于0以免引入噪声？只是原文说到做实验发现这样子效果好，所以得思考一下)
- 文章将DR的整个过程看做迭代过程
- 作者对整个DR过程分析发现，如果使用$b_{ij}=b_{ij}+u_i \cdot v_j$，经过无穷次迭代后softmax的值非零即一，也就是每个底层的胶囊仅仅联系到唯一一个上层胶囊。这时候 vj 已不再是聚类中心，而是距离它们聚类中心最近的那个ui。作者觉得很不合理。不同的类别之间是有可能有共同的特征的，这就好比人和动物虽然不一样，但是都有眼睛。
作者无法忍受这种理论上的不足，所以将其改为
$b_{ij}=u_i \cdot v_j$
- 在底层胶囊传入下一个胶囊前，都需要乘上矩阵*W*，以实现“多角度看特征”。将每个变换矩阵看成是上层胶囊的识别器，上层胶囊通过这个矩阵来识别出底层胶囊是不是有这个特征。
- 做了什么：提供了一种新的、基于聚类思想来代替池化完成特征的整合的方案，这种新方案的特征表达能力更加强大。
- 事实上，通过向量的模长来表示概率，这一点让我想起了量子力学的波函数，它也是通过波函数的范数来表示概率的。这告诉我们，未来 Capsule 的发展也许可以参考一下量子力学的内容。
- **反向传播好不好？** 求梯度，就是一种比重复试探更加高明的技巧，何乐而不用呢？
- **池化好不好？** 所以我认为池化也是可取的，不过池化应该对低层的特征进行，高层的信息池化可能就会有问题了。


---

## 再来一顿贺岁宴 | 从K-Means到Capsule

- 来源：[here](https://mp.weixin.qq.com/s?__biz=MzIwMTc4ODE0Mw==&mid=2247487194&idx=1&sn=1be48dd9a45f7f3a35024fdce55f2369&chksm=96e9d15aa19e584c0b4815d6b5791a7ab859049116f8500a08c454c55457f9489cf2adf4ab93&scene=21%23wechat_redirect&pass_ticket=lmL8VwvCE9islElDAChRseZPB%2FtG9VYF%2FCRr82%2BET8Qx7hnfXx%2FjJMHlLsFfeCis)

1. 模型应该自己会分解和组合，理解新事物，而不是像现在的深度学习模型，针对性太强
2. 希望特征是一种分布式表示
3. 通过聚类组合特征
4. 两个字为什么能成为一个词，是因为这两个字经常扎堆出现，而这个堆只有他们俩。类比到特征，就是说特征的聚合是因为他们有聚类倾向（我的理解是，两个特征如果在其空间中比较近，那么就比较相似。有聚类倾向这种说法有些不妥）
5. 由于capsule v是由底层capsule u加权相加得到，所以v的模长越大，说明越多“小弟”
