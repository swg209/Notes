1. 搞清楚数据的含义
2. 搞清楚需要构建怎样的数据格式
3. 搞清楚gcn的结构，其需要的参数，shape
4. 搞清楚capsule的结构，其需要的参数，shape
5. 将上面串联起来
6. 后面加存储、输出等东西


## 原始数据

共 3312 行，每行3703个特征


## gcn

adj [3327, 3327]
features [3327,3703]

## 效果比较差原因分析

1. 搞清楚loss和acc有没有写错
2. 上面没问题还是差的话，使capsule层直接输出结果，更改loss


## 结果

| experiment | result|
| --- | ---|
| gcn+gcn relu+x | Test set results: cost= 0.93359 accuracy= 0.78900 time= 0.03449 |
| gcn+gcn relu*2 | Test set results: cost= 0.86732 accuracy= 0.79900 time= 0.03543 |
| gcn | Test set results: cost= 1.65261 accuracy= 0.70500 time= 0.01599 |
| gcn-capsule | Test set results: cost= 0.86282 accuracy= 0.14392 time= 29.06506 |
| gcn-capsule-gcn | 结果与gcn-capsule差不多 |


## Batch normalization

由于要和Laplacian相乘，如果做batchnorm的话要对节点做onehot，目前觉得有点麻烦，所以先停了


## erroe


---

# 新篇章

目的：分析为何capsule能够取得好效果，通过对比找出gcn+capsule效果不好的原因

### 实验

1. 在conv后面加fc，观察不加capsule结果能有多好
2. 通过可视化的手段观察变量的汇聚情况

### 原作者代码阅读

**relu conv是常规的**

```python
kernel = variables.weight_variable(
          shape=[9, 9, image_depth, 256], stddev=5e-2,
          verbose=self._hparams.verbose)
biases = variables.bias_variable([256], verbose=self._hparams.verbose)
```

**conv_slim_capsule是这样的**

```python
kernel = variables.weight_variable(shape=[
        kernel_size, kernel_size, input_atoms, output_dim * output_atoms
    ])
biases = variables.bias_variable([output_dim, output_atoms, 1, 1])
```

作者将relu conv的“厚度”作为图像的dim，output_dim也设置为256，然后再reshape成32*8

**这一层的中间结果是votes，shape如下**

`[batch, input_dim, output_dim, output_atoms, out_height, out_width]`

接下来就是routing的内容

每个routing只做了一次

### 不同点

relu_conv to PrimaryCaps使用了routing，routing只迭代了一轮。
