1. 搞清楚数据的含义
2. 搞清楚需要构建怎样的数据格式
3. 搞清楚gcn的结构，其需要的参数，shape
4. 搞清楚capsule的结构，其需要的参数，shape
5. 将上面串联起来
6. 后面加存储、输出等东西


## 原始数据

共 3312 行，每行3703个特征


## gcn

adj [3327, 3327]
features [3327,3703]

## 效果比较差原因分析

1. 搞清楚loss和acc有没有写错
2. 上面没问题还是差的话，使capsule层直接输出结果，更改loss


## 结果

| experiment | result|
| --- | ---|
| gcn+gcn relu+x | Test set results: cost= 0.93359 accuracy= 0.78900 time= 0.03449 |
| gcn+gcn relu*2 | Test set results: cost= 0.86732 accuracy= 0.79900 time= 0.03543 |
| gcn | Test set results: cost= 1.65261 accuracy= 0.70500 time= 0.01599 |
| gcn-capsule | Test set results: cost= 0.86282 accuracy= 0.14392 time= 29.06506 |
| gcn-capsule-gcn | 结果与gcn-capsule差不多 |


## Batch normalization

由于要和Laplacian相乘，如果做batchnorm的话要对节点做onehot，目前觉得有点麻烦，所以先停了


## erroe


---

# 新篇章

目的：分析为何capsule能够取得好效果，通过对比找出gcn+capsule效果不好的原因

### 实验

1. 在conv后面加fc，观察不加capsule结果能有多好
2. 通过可视化的手段观察变量的汇聚情况

### 原作者代码阅读

**relu conv是常规的**

```python
kernel = variables.weight_variable(
          shape=[9, 9, image_depth, 256], stddev=5e-2,
          verbose=self._hparams.verbose)
biases = variables.bias_variable([256], verbose=self._hparams.verbose)
```

**conv_slim_capsule是这样的**

```python
kernel = variables.weight_variable(shape=[
        kernel_size, kernel_size, input_atoms, output_dim * output_atoms
    ])
biases = variables.bias_variable([output_dim, output_atoms, 1, 1])
```

作者将relu conv的“厚度”作为图像的dim，output_dim也设置为256，然后再reshape成32*8

**这一层的中间结果是votes，shape如下**

`[batch, input_dim, output_dim, output_atoms, out_height, out_width]`

接下来就是routing的内容

每个routing只做了一次

### 不同点

relu_conv to PrimaryCaps使用了routing，routing只迭代了一轮。



评价指标 accuracy
|  | kmeans++(cosin) | kmeans++(euclidean) | gcn |
| :---: | :---: | :---: | :---: |
| 原始数据 | 0.44 | 0.33 |  0.66 |
| 经过一层gcn后 | 0.74 | 0.55 | 0.78 |


评价指标 accuracy
|  | cora(2708) | citeseer(3327) | pubmed(19717) |
| :---: | :---: | :---: | :---: |
| (半监督学习)gcn+gcn | 0.78200 | 0.69600 | 0.74800 |
| (半监督学习)gcn+capsule | 0.63600 | 0.59600 | 0.67600 |
| (监督学习)gcn+gcn | 0.86600 | 0.77600 |0.86600 |
| (监督学习)gcn+capsule | 0.72400 | 0.70900 | 0.89400 |

cora(2708)表示cora数据集，共有2708条数据

实验结果显示，随着数据量的增多，gcn+capsule的效果变好。


---

capsule作为中间层

cora数据集

Epoch: 0200 train_loss= 0.00045 train_acc= 1.00000 val_loss= 0.32718 val_acc= 0.34400 time= 33.65158
Test set results: cost= 0.63688 accuracy= 0.35800 time= 12.69094
Optimization Finished!
Test set results: cost= 0.63688 accuracy= 0.35800 time= 13.00009


---

| 数据集 | cap-iter | hidden1 | output_dim | best-epoch | test-acc | 备注 |
| :---: | :---: | :---: | :---: | :---: | :---: | :---: |
| cora | 1 | 200 | 20 | 51 | 0.8280008 |
| cora | 2 | 200 | 20 | 33 | 0.7760006 |
| cora | 3 | 200 | 20 | 54 | 0.75600 |
| cora | 5 | 200 | 20 | 46 | 0.732 |
| cora | 1 | 300 | 20 | 45 | 0.8240008 |
| cora | 1 | 100 | 20 | 81 | 0.83100075 |
| cora | 1 | 50 | 20 | 96 | 0.8290008 |
| cora | 1 | 100 | 10 | 96 | 0.8440008 |
| cora | 1 | 100 | 5 | 94 | 0.7940007 |
| cora | 1 | 100 | 10 | 119 | 0.86100084 | 去掉l2正则，改用cross_entropy |
| citeseer | 1 | 100 | 10 | 77 | 0.7850009 | 去掉l2正则，改用cross_entropy |
| pubmed | 1 | 100 | 10 | 351 | 0.8960005 | 去掉l2正则，改用cross_entropy |
| pubmed | 3 | 100 | 10 | 232 | 0.9040004 | 去掉l2正则，改用cross_entropy |
