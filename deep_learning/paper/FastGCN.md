
与许多图论算法一样，邻接矩阵对训练和测试数据进行成对编码。模型的学习和嵌入是同时进行的，至少是作者提出的。然而，对于许多应用程序来说，测试数据可能并不容易获得，因为图可能会随着新的顶点不断地扩展(例如，测试数据可能会随着新的顶点不断地扩展)。社交网络的新成员，推荐系统的新产品，功能测试的新药物)。这种情况需要一种归纳模式，这种模式只从一组顶点学习模型，并能很好地推广到图的任何扩充。


GCN面临的一个更严峻的挑战是，在批处理训练中，跨层递归扩展邻域会导致昂贵的计算。特别是对于稠密图和幂律图，单个顶点的邻域扩展很快就会占据图的很大一部分。然后，通常的小批处理培训将涉及到每个批处理的大量数据，即使是小批处理。因此，要使GCN适用于大型密集图，可伸缩性是一个迫切需要解决的问题。为了解决这两个难题，我们提出从不同的角度来看待图形卷积，并将其解释为概率度量下嵌入函数的积分变换。这种观点为归纳学习提供了一种原则性的机制，从损失的公式化到梯度的随机化。特别地，我们解释图的顶点是一些概率分布的iid样本，并把损失和每个卷积层写成关于顶点嵌入函数的积分。然后，通过蒙特卡罗近似计算积分，蒙特卡罗近似定义了样本损失和样本梯度。可以进一步改变抽样分布(如重要性抽样)以减少近似方差。该方法不仅消除了对测试数据的依赖，而且每批计算的成本可控。在撰写本文时，我们注意到一篇新发表的作品GraphSAGE (Hamilton et al.， 2017)也建议使用抽样来减少GCN的计算足迹。我们的采样方案更经济，在梯度计算中节省了大量的时间，这将在3.3节中详细分析。第4节的实验结果表明，FastGCN的每批计算速度比GraphSAGE快一个数量级以上，分类准确率具有高度可比性


在过去的几年里，一些基于图形的卷积网络模型出现在了图形结构化数据寻址应用中，例如分子的表示(Duvenaud et al.，2015)。一个重要的工作流程是建立在谱图理论(Bruna et al.， 2013;Henaffet, 2015;Defferrard等，2016)。它们在谱域中定义参数化滤波器，由图的傅里叶变换实现。这些方法学习了整个图的特征表示，可以用于图的分类。另一项工作是学习图形顶点的嵌入，Goyal & Ferrara(2017)是最近的一项调查，涵盖了多种方法。一种主要的基于矩阵分解的嵌入算法的分类一致性;参见，例如Roweis & Saul (2000);Belkin & Niyogi (2001);Ahmed et al. (2013);曹等(2015);Ou et al。(2016)。这些方法共同学习训练和测试数据的表示。另一类是基于随机游走的方法(Perozzi et al.， 2014;(Grover & Leskovec, 2016)通过对社区的探索计算节点表示。LINE (Tang et al.， 2015)也是这样一种以保留一级和二级代理为动机的技术nique。同时，也出现了一些深度神经网络架构，如SDNE (Wang et al.， 2016)，较好地捕捉了图内的非线性。如前所述，GCN (Kipf & Welling, 2016a)是我们工作的基础模型。与我们的方法最相关的工作是GraphSAGE (Hamilton et al.， 2017)，它通过收集邻居信息来学习节点表示。其中一个提议的聚合器采用GCN架构。作者也认识到GCN的内存瓶颈，因此提出了一种特殊的采样方案来限制邻域大小。我们的抽样方法是基于一个不同的、更有原则的公式。主要的区别是我们采样的是顶点而不是邻居。计算结果的节省将在3.3节中分析。

## 3 TRAINING ANDINFERENCE THROUGHSAMPLING

GCN与许多标准神经网络架构的一个显著区别是样本丢失不依赖。基于损失函数对独立数据样本的可加性，设计了SGD及其批量泛化训练算法。另一方面，对于图来说，每个顶点都与它所有的邻居进行卷积，因此定义一个足够的梯度来计算是非常简单的。

具体地说，考虑标准SGD场景，其中损失是某个函数 $g$ 对数据分布 $D$ 的期望

$$L= E_{x-D}[g(W;x)], x-D$$

- $D$ 是某个数据分布
- $x$ 是从 $D$ 抽样出来的

在SGD的每一步，梯度近似为rg(W;x_i)，一个(假设)无偏样本。可以解释为，每个梯度步骤都朝着样本损耗(W;习)的方向前进。样本损失和样本梯度只涉及一个样本。对于图，人们可能不再利用独立性，通过递归地丢弃ofi的相邻顶点及其相邻点的信息来计算样本梯度中心(W;习)。因此，我们寻求另一种提法。为了将学习问题转换到相同的采样框架下，我们假设存在一个顶点setv0与概率空间(V0;F;P)相关联的图(可能是无限的)，对于给定的图，它是g0的一个诱导子图，其顶点是根据概率度量的V0的iid样本。对于概率空间，v0作为样本空间，f可以是任意事件空间(例如，thepower setF= 2V0)。概率测量定义了抽样分布。为了解决卷积导致的缺乏独立性的问题，我们解释了网络的每一层都定义了一个顶点(随机变量)的嵌入函数，这些顶点(随机变量)与相同的概率测度有关，但它们是独立的。参见图1。具体来说，回想一下gcn的架构
