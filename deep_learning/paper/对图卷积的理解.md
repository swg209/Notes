http://www.cnblogs.com/yangperasd/p/7071657.html

我们首先不介绍graph conv的理论。我们首先介绍如何使用graph conv。

首先，我们知道图 G =（V，E），其中X表示顶点集V的特征，A表示图的结构信息，通常使用邻接矩阵。在一层的graph conv中，使用上层的输出Hl，A和本层的W作为输入，经过某种函数映射f后，便可以得到本层的输出。

下面，我们假设使用如图所示的函数σ。由于A是图的邻接矩阵，所以只有在当前点与其他点有连接的时候才会有值。这样AHW就会表示当前节点的所有邻居在上一层的输出乘以W。这样，我们通过函数σ就仅仅看到了当前点的局部连接。这与conv的局部连接非常相似。因此，我们可以从这一点来理解graph conv。同时，当我们使用多层的graph conv时，H2会利用H1的值，H1利用的是当前节点的1介邻居的信息，而H2便是利用当前节点1介和2介邻居的信息。


我们利用刚才函数的复杂版，在karate-club数据集上，随机初始化W，使用3层的graph conv，将最后的H3输出来，便可以得到如图的结果。可以看到在未训练时，节点之间的向量距离还不错（相同颜色的点距离较近）。

图中Ahat=A+I，Dhat表示Ahat节点度的对角矩阵。

在理论上，我们可以通过两种途径来解释graph conv。

- 在频谱图理论中，卷积可以表示为矩阵的乘积。将该公式-4运用chebyshev多项式和其他的近似，我们可以得到公式-5.而公式-5 与我们刚才使用的函数σ是基本相同的。

- W-L算法告诉我们，我们可以使用当前节点的邻居表示它。
