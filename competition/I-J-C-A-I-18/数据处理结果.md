## instance_id

我将train和test结合在一起，置test的'is_trade'=2，然后统计instance_id，发现train里面出现的instance_id，也会在test里面出现，大概有3条。可以根据这个直接给test中重复的instance_id设'is_trade'

## item_category_list

```python
split(';')

train_max_len = 3
test_max_len = 3
```

## item_property_list

```python
split(';')

train_max_len = 100
test_max_len = 100
```

## predict_category_property

```python
split(';')

train_max_len = 14
test_max_len = 12
```

## context_timestamp

更新：已将timestamp转化成月日时分

```python
-----------
train&test
-----------

print((timestamp_max - timestamp_min)/60/60/24)

7.99958333333

对context_timestamp按时间划分进行统计

day_list = [0 for i in range(8)]
for item in data['context_timestamp']:
    day = (item - timestamp_min)//86400
    day_list[day] += 1
print(day_list)

[78268, 70931, 68387, 71199, 68318, 63616, 57419, 18371]

-----------
train
-----------
6.99983796296

该天的数据量： [78268, 70931, 68387, 71199, 68318, 63616, 57419]
转化成功：    [1567, 1396, 1324, 1375, 1289, 1099, 971]
总数据：    [78268, 70931, 68387, 71199, 68318, 63616, 57419]
转化率：    [0.020, 0.020, 0.019, 0.019, 0.019, 0.017, 0.017]

------------
猜测
------------
1. 最后一天test只有18371条数据
2. 第一第二天应该是周末
3.
```

## 所有id作count

有一个问题，到底要不要统计test的数据？算是数据泄露吗？


## 模型运行结果

#### xgboost

```python
params = {'booster': 'gbtree',
              'objective': 'binary:logistic',
              'eval_metric': 'auc',
              'gamma': 0.1,
              'min_child_weight': 1.1,
              'max_depth': 7,
              'lambda': 10,
              'subsample': 0.7,
              'colsample_bytree': 0.7,
              'colsample_bylevel': 0.7,
              'eta': 0.01,
              'tree_method': 'exact',
              'seed': 123,
              'nthread': 12
              }

[1461]	train-auc:0.868962	val-auc:0.660864
Stopping. Best iteration:              
[1161]	train-auc:0.842845	val-auc:0.662198

令max_depth=5也差不多

--------------------------
train 012345 val 6
[737]	train-logloss:0.08924	val-logloss:0.082714


--------------------------
train1 012 val1 3
train2 123 val2 4
train3 234 val3 5

[1186]	train1-auc:0.841679	val1-auc:0.67162
[861]	train2-auc:0.810119	val2-auc:0.682462
[1387]	train3-auc:0.867984	val3-auc:0.664714

train1 0123 val1 4
train2 1234 val2 5
train3 2345 val3 6

[912]	train1-auc:0.796563	val1-auc:0.685643
[1479]	train2-auc:0.852829	val2-auc:0.666025
[1024]	train3-auc:0.81983	val3-auc:0.662747

[622]	train1-logloss:0.089073	val1-logloss:0.089618
[646]	train2-logloss:0.087347	val2-logloss:0.084214
[690]	train3-logloss:0.084551	val3-logloss:0.082746

可以看到，增加了一天训练数据总体来说没有太大变化
--------------------------
```

---
对所有的天数作了count然后划分，logloss有所下降，但是不清楚这会不会造成数据泄露
```python
[698]	train1-logloss:0.086641	val1-logloss:0.088594
[644]	train2-logloss:0.08605	val2-logloss:0.083309
[707]	train3-logloss:0.083111	val3-logloss:0.081886
```
---
在上面的基础上

- 将xgb的max_depth改为5
- timestamp转换成月日时分

然后进行训练，又有了提升。还有就是，迭代次数变长了很多


```python
[1748]	train1-logloss:0.085341	val1-logloss:0.087755
[1884]	train2-logloss:0.083636	val2-logloss:0.082524
[1565]	train3-logloss:0.082406	val3-logloss:0.08154
```

然后

- 将xgb的max_depth改为7

```python
[1319]	train1-logloss:0.081184	val1-logloss:0.087723
[1167]	train2-logloss:0.081019	val2-logloss:0.082625
[1065]	train3-logloss:0.079552	val3-logloss:0.081583
```

所以还是将max_depth设为5比较好

---

**样本不平衡问题**

对数据进行了采样，分别是对负样本以正样本倍数进行采样，和复制正样本以增加正样本，两种方法都带来了模型效果的巨大下降，并且3000轮以内不收敛。

```python
utils.balance_sample
negk = 15, posk = 2

[2999]	train1-logloss:0.301241	val1-logloss:0.148642
[2999]	train2-logloss:0.300293	val2-logloss:0.145512
[2999]	train3-logloss:0.298642	val3-logloss:0.141575
```

为了确定是不是效果到底有没有提升，我将迭代次数设置为10000+

**当然，效果不好不能说明样本不平衡对模型没有影响，可能其他除了样本不平衡问题的方法会有效，还有model可能对数据不平衡不敏感**

```python
Stopping. Best iteration:
[8137]	train1-logloss:0.264357	val1-logloss:0.14582

```
---

```python
max_depth = 6

train_O_SC_C_day1718192021.csv

train 17-21     test 22 23 24
train 18-22     test 23 24
train 19-23     test 24
[1539]	train1-logloss:0.082847	val1-logloss:0.081145	val2-logloss:0.089003	val3-logloss:0.082183
[1502]	train2-logloss:0.083501	val2-logloss:0.082355	val3-logloss:0.082122
[2999]	train3-logloss:0.075191	val3-logloss:0.069325

最后一个不收敛
```
---
```python
组合特征

[1382]	train-logloss:0.083446	val1-logloss:0.081969	val2-logloss:0.08084
```

---


## Todo

- 对`'item_category_list'`作统计，因为可能某些类目(消耗品)购买的可能性比较高
- `'item_property_list'`有相当的多...目前并不知道这个属性的准确意义，打算对list中按权重加权相加构成新特征，把这个特征去掉
- `'predict_category_property'`
- 看到技术圈有人说。这个现象值得关注？
  `'item_id'`中test在train出现过的个数:18164
  `'item_id'`中test没有在train出现过的个数:207
- 为什么会有一个predict_catagory_property字段？不是和已经有 item_catagory和item_property_list了吗
- 计算广告的ctr作为特征
- 统计用户的活跃时间、app的活跃时间
- 初步分析：上图是一个用户对一个时间窗口内的app的考察记录，app之间的箭头表示用户点击之间的跳转，在整个过程中，我们可以计算每个app的入度，这样可以找到哪个app更受此用户欢迎。
- 可能需要对负样本进行采样以解决样本不平衡的问题


## 官方关于数据的说明

- 每个广告商品都有类目（category）和属性（property）的字段。一个广告商品，有从大到小的若干个类目，以及没有从属关系的若干个属性。每个广告商品的类目数量不固定，属性数量也不固定。举例说明，某一个广告商品的 item_category_list 可能是“手机;全面屏手机”；这个广告商品的item_property_list 可能是“白色;支持LTE;4.7英寸屏幕”。

- predict_category_property 是根据上下文信息推测出的类目和属性，其中的每一个预测类目对应于若干个（可能为零个）预测属性。举例说明，某一个样本的 predict_category_property 可能对应于“手机:白色,支持LTE,4.7英寸屏幕;水果:红色,球形;公司:-1”，其中 -1 表示预测类目“公司”没有找到适用于当前广告商品的预测属性。

- context_timestamp指的是广告被点击的那个时间点
